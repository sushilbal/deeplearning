{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network class in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Introduction and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('TensorFlow Version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Initializing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        self.num_features = layers[0]\n",
    "        self.num_classes = layers[-1]\n",
    "        \n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        \n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        \n",
    "        self.setup()\n",
    "    \n",
    "        \n",
    "    def setup(self):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i] = tf.Variable(tf.random.normal(shape = (self.layers[i], self.layers[i-1]))) # initialize weight\n",
    "            self.b[i] = tf.Variable(tf.random.normal(shape=(self.layers[i], 1))) # initialize bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def forward_pass(self, X):\n",
    "        A = tf.convert_to_tensor(X, dtype = tf.float32) \n",
    "        for i in range(1, self.L):\n",
    "            # where Z is linear output\n",
    "            Z = tf.matmul(A, tf.transpose(self.W[i])) + tf.transpose(self.b[i])\n",
    "            if i != self.L-1:\n",
    "                A = tf.nn.relu(Z)\n",
    "            else:\n",
    "                A = Z \n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Computing Loss & Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def compute_loss(self,A,Y):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(Y,A)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def update_params(self, lr):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i].assign_sub(lr * self.dW[i])\n",
    "            self.b[i].assign_sub(lr * self.db[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict & Info Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def predict(self, X):\n",
    "        A = self.forward_pass(X)\n",
    "        return tf.argmax(tf.nn.softmax(A), axis=1)\n",
    "    \n",
    "    def info(self):\n",
    "        num_params = 0\n",
    "        for i in range(1, self.L):\n",
    "            num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
    "            num_params += self.b[i].shape[0]\n",
    "        print('Input Features:', self.num_features)\n",
    "        print('Number of Classes:', self.num_classes)\n",
    "        print('Hidden Layers:')\n",
    "        print('--------------')\n",
    "        for i in range(1, self.L-1):\n",
    "            print('Layer {}, Units {}'.format(i, self.layers[i]))\n",
    "        print('--------------')\n",
    "        print('Number of parameters:', num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Training on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def train_on_batch(self, X, Y, lr):\n",
    "        X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            A = self.forward_pass(X)\n",
    "            loss = self.compute_loss(A,Y)\n",
    "        for i in range(1, self.L):\n",
    "            self.dW[i] = tape.gradient(loss, self.W[i])\n",
    "            self.db[i] = tape.gradient(loss, self.b[i])\n",
    "        del tape\n",
    "        self.update_params(lr) #update learning rate\n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Training on complete set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr):\n",
    "        # Keeping track of validation loss, training loss & validation accuracy\n",
    "        history = {\n",
    "            'val_loss': [],\n",
    "            'train_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        for e in range(0, epochs):\n",
    "            epoch_train_loss = 0.\n",
    "            print('Epoch {}'.format(e), end='.')\n",
    "            for i in range(0, steps_per_epoch):\n",
    "                x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                batch_loss = self.train_on_batch(x_batch, y_batch, lr)\n",
    "                epoch_train_loss += batch_loss\n",
    "                \n",
    "                if i%int(steps_per_epoch/10) == 0:\n",
    "                    print(end='.')\n",
    "                    \n",
    "            history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
    "            val_A = self.forward_pass(x_test)\n",
    "            val_loss = self.compute_loss(val_A, y_test).numpy()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            val_preds = self.predict(x_test)\n",
    "            val_acc = np.mean(np.argmax(y_test, axis = 1) == val_preds.numpy())\n",
    "            history['val_acc'].append(val_acc)\n",
    "            print('Validation Accuracy:', val_acc)\n",
    "            \n",
    "        return history\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_examples(x, y, p=None):\n",
    "    indices = np.random.choice(range(0, x.shape[0]), 10)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    if p is None:\n",
    "        p = y\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, index in enumerate(indices):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(x[index].reshape((28, 28)), cmap='binary')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if y[index] == p[index]:\n",
    "            col = 'g'\n",
    "        else:\n",
    "            col = 'r'\n",
    "        plt.xlabel(str(p[index]), color=col)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_examples(x_train, y_train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([784, 128, 128, 10])\n",
    "net.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=120\n",
    "epoch=5\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "lr = 3e-3\n",
    "print('Steps per epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = net.train(\n",
    "    x_train, y_train,\n",
    "    x_test, y_test,\n",
    "    epoch, steps_per_epoch,\n",
    "    batch_size, lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical Learning Rate: https://arxiv.org/pdf/1506.01186.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
